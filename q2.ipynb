{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, ngf, nc):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 2, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nc, ndf):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100  # Size of the latent vector\n",
    "ngf = 64  # Number of generator filters\n",
    "ndf = 64  # Number of discriminator filters\n",
    "nc = 3    # Number of channels in the input images (RGB)\n",
    "\n",
    "generator = Generator(latent_dim, ngf, nc)\n",
    "discriminator = Discriminator(nc, ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizerG = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "num_epochs = 10 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[0/10][0/782] Loss_D: 1.4998 Loss_G: 1.4605\n",
      "[0/10][100/782] Loss_D: 0.0766 Loss_G: 5.2175\n",
      "[0/10][200/782] Loss_D: 0.6036 Loss_G: 2.9205\n",
      "[0/10][300/782] Loss_D: 0.7869 Loss_G: 2.5276\n",
      "[0/10][400/782] Loss_D: 0.7677 Loss_G: 2.2605\n",
      "[0/10][500/782] Loss_D: 0.5334 Loss_G: 3.8287\n",
      "[0/10][600/782] Loss_D: 0.7242 Loss_G: 2.4000\n",
      "[0/10][700/782] Loss_D: 0.5402 Loss_G: 3.1009\n",
      "[1/10][0/782] Loss_D: 1.0064 Loss_G: 2.0091\n",
      "[1/10][100/782] Loss_D: 0.5122 Loss_G: 2.8126\n",
      "[1/10][200/782] Loss_D: 0.4616 Loss_G: 2.7538\n",
      "[1/10][300/782] Loss_D: 0.5534 Loss_G: 2.7803\n",
      "[1/10][400/782] Loss_D: 0.7853 Loss_G: 3.9533\n",
      "[1/10][500/782] Loss_D: 0.6132 Loss_G: 2.9005\n",
      "[1/10][600/782] Loss_D: 0.2302 Loss_G: 2.8824\n",
      "[1/10][700/782] Loss_D: 0.3411 Loss_G: 2.8650\n",
      "[2/10][0/782] Loss_D: 0.1878 Loss_G: 2.9250\n",
      "[2/10][100/782] Loss_D: 0.5352 Loss_G: 4.3931\n",
      "[2/10][200/782] Loss_D: 0.3245 Loss_G: 3.1863\n",
      "[2/10][300/782] Loss_D: 0.5533 Loss_G: 3.5962\n",
      "[2/10][400/782] Loss_D: 0.6483 Loss_G: 4.5360\n",
      "[2/10][500/782] Loss_D: 0.5253 Loss_G: 2.2697\n",
      "[2/10][600/782] Loss_D: 0.2432 Loss_G: 2.7049\n",
      "[2/10][700/782] Loss_D: 0.6019 Loss_G: 2.8215\n",
      "[3/10][0/782] Loss_D: 0.1894 Loss_G: 3.4688\n",
      "[3/10][100/782] Loss_D: 0.5650 Loss_G: 3.1270\n",
      "[3/10][200/782] Loss_D: 0.4833 Loss_G: 3.1700\n",
      "[3/10][300/782] Loss_D: 0.5387 Loss_G: 1.9325\n",
      "[3/10][400/782] Loss_D: 0.4401 Loss_G: 2.7954\n",
      "[3/10][500/782] Loss_D: 0.6054 Loss_G: 4.7524\n",
      "[3/10][600/782] Loss_D: 0.9382 Loss_G: 4.0738\n",
      "[3/10][700/782] Loss_D: 0.2399 Loss_G: 2.5571\n",
      "[4/10][0/782] Loss_D: 0.2696 Loss_G: 3.5837\n",
      "[4/10][100/782] Loss_D: 0.5832 Loss_G: 4.2688\n",
      "[4/10][200/782] Loss_D: 0.2177 Loss_G: 2.9082\n",
      "[4/10][300/782] Loss_D: 0.5405 Loss_G: 4.5216\n",
      "[4/10][400/782] Loss_D: 0.6105 Loss_G: 4.1019\n",
      "[4/10][500/782] Loss_D: 0.5465 Loss_G: 2.1674\n",
      "[4/10][600/782] Loss_D: 0.4906 Loss_G: 2.5094\n",
      "[4/10][700/782] Loss_D: 0.1503 Loss_G: 3.1045\n",
      "[5/10][0/782] Loss_D: 0.1706 Loss_G: 3.2739\n",
      "[5/10][100/782] Loss_D: 0.1476 Loss_G: 4.4075\n",
      "[5/10][200/782] Loss_D: 0.4969 Loss_G: 3.5738\n",
      "[5/10][300/782] Loss_D: 0.2543 Loss_G: 2.8939\n",
      "[5/10][400/782] Loss_D: 0.1513 Loss_G: 4.1276\n",
      "[5/10][500/782] Loss_D: 0.3014 Loss_G: 3.3403\n",
      "[5/10][600/782] Loss_D: 0.1635 Loss_G: 4.1351\n",
      "[5/10][700/782] Loss_D: 0.2510 Loss_G: 3.5767\n",
      "[6/10][0/782] Loss_D: 0.5493 Loss_G: 4.1989\n",
      "[6/10][100/782] Loss_D: 0.2158 Loss_G: 2.4316\n",
      "[6/10][200/782] Loss_D: 0.4472 Loss_G: 2.3465\n",
      "[6/10][300/782] Loss_D: 0.1676 Loss_G: 3.6224\n",
      "[6/10][400/782] Loss_D: 0.4877 Loss_G: 2.6844\n",
      "[6/10][500/782] Loss_D: 0.5186 Loss_G: 3.3445\n",
      "[6/10][600/782] Loss_D: 0.1513 Loss_G: 3.6219\n",
      "[6/10][700/782] Loss_D: 0.5631 Loss_G: 4.9589\n",
      "[7/10][0/782] Loss_D: 0.2228 Loss_G: 3.8057\n",
      "[7/10][100/782] Loss_D: 0.1728 Loss_G: 4.3322\n",
      "[7/10][200/782] Loss_D: 0.3137 Loss_G: 3.7001\n",
      "[7/10][300/782] Loss_D: 0.1752 Loss_G: 3.7465\n",
      "[7/10][400/782] Loss_D: 0.0657 Loss_G: 4.4520\n",
      "[7/10][500/782] Loss_D: 0.9740 Loss_G: 7.6182\n",
      "[7/10][600/782] Loss_D: 0.1937 Loss_G: 4.0490\n",
      "[7/10][700/782] Loss_D: 0.1291 Loss_G: 3.8168\n",
      "[8/10][0/782] Loss_D: 0.0646 Loss_G: 4.5329\n",
      "[8/10][100/782] Loss_D: 0.1479 Loss_G: 3.8339\n",
      "[8/10][200/782] Loss_D: 0.2401 Loss_G: 3.1911\n",
      "[8/10][300/782] Loss_D: 0.0785 Loss_G: 3.8933\n",
      "[8/10][400/782] Loss_D: 0.2869 Loss_G: 4.7486\n",
      "[8/10][500/782] Loss_D: 0.1416 Loss_G: 3.2527\n",
      "[8/10][600/782] Loss_D: 0.1804 Loss_G: 3.3388\n",
      "[8/10][700/782] Loss_D: 0.1932 Loss_G: 3.7286\n",
      "[9/10][0/782] Loss_D: 0.1754 Loss_G: 3.9412\n",
      "[9/10][100/782] Loss_D: 0.1488 Loss_G: 4.3268\n",
      "[9/10][200/782] Loss_D: 0.1549 Loss_G: 3.6772\n",
      "[9/10][300/782] Loss_D: 0.3781 Loss_G: 3.5082\n",
      "[9/10][400/782] Loss_D: 0.2137 Loss_G: 2.9368\n",
      "[9/10][500/782] Loss_D: 0.2496 Loss_G: 3.2145\n",
      "[9/10][600/782] Loss_D: 0.1016 Loss_G: 3.9917\n",
      "[9/10][700/782] Loss_D: 0.3228 Loss_G: 3.3554\n"
     ]
    }
   ],
   "source": [
    "loss_D=[]\n",
    "loss_G=[]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_each_D=[]\n",
    "    loss_each_G=[]\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        discriminator.zero_grad()\n",
    "        real_data, _ = data\n",
    "        batch_size = real_data.size(0)\n",
    "        label = torch.full((batch_size,), 1.0)\n",
    "        output = discriminator(real_data)\n",
    "        errD_real = criterion(output.view(-1), label)\n",
    "        errD_real.backward()\n",
    "        noise = torch.randn(batch_size, latent_dim, 1, 1)\n",
    "        fake_data = generator(noise)\n",
    "        label.fill_(0.0)\n",
    "        output = discriminator(fake_data.detach())\n",
    "        errD_fake = criterion(output.view(-1), label)\n",
    "        errD_fake.backward()\n",
    "\n",
    "        errD=errD_fake+errD_real\n",
    "        loss_each_D.append(errD.item())\n",
    "        optimizerD.step()\n",
    "\n",
    "        generator.zero_grad()\n",
    "        label.fill_(1.0)  \n",
    "        output = discriminator(fake_data)\n",
    "        errG = criterion(output.view(-1), label)\n",
    "        errG.backward()\n",
    "        loss_each_G.append(errG.item())\n",
    "        optimizerG.step()\n",
    "\n",
    "        \n",
    "        print(\"EPOCH\",epoch,\"Loss_D\",errD.item(),\"Loss_G\",errG.item())\n",
    "    \n",
    "    loss_D.append(sum(loss_each_D)/len(loss_each_D))\n",
    "    loss_G.append(sum(loss_each_G)/len(loss_each_G))\n",
    "\n",
    "torch.save(generator.state_dict(), 'generator.pth')\n",
    "torch.save(discriminator.state_dict(), 'discriminator.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_D, label='Discriminator Loss')\n",
    "plt.plot(loss_G, label='Generator Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.makedirs('images', exist_ok=True)\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        noise = torch.randn(1, latent_dim, 1, 1)\n",
    "        fake = generator(noise).detach().cpu()\n",
    "        plt.figure(figsize=(1,1))\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(np.transpose(fake[0],(1,2,0)))\n",
    "        plt.savefig('images/fake_image_%d.png' % i)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "from torchvision.models.inception import inception_v3\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def inception_score(imgs, cuda=False, batch_size=32, resize=False, splits=1):\n",
    "    \"\"\"Computes the inception score of the generated images imgs\n",
    "\n",
    "    imgs -- Torch dataset of (3xHxW) numpy images normalized in the range [-1, 1]\n",
    "    cuda -- whether or not to run on GPU\n",
    "    batch_size -- batch size for feeding into Inception v3\n",
    "    splits -- number of splits\n",
    "    \"\"\"\n",
    "    N = len(imgs)\n",
    "    # print(imgs)\n",
    "\n",
    "    assert batch_size > 0\n",
    "    assert N > batch_size\n",
    "\n",
    "    # Set up dtype\n",
    "    if cuda:\n",
    "        dtype = torch.cuda.FloatTensor\n",
    "    else:\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"WARNING: You have a CUDA device, so you should probably set cuda=True\")\n",
    "        dtype = torch.FloatTensor\n",
    "\n",
    "    # Set up dataloader\n",
    "    dataloader = torch.utils.data.DataLoader(imgs, batch_size=batch_size)\n",
    "\n",
    "    # Load inception model\n",
    "    inception_model = inception_v3(pretrained=True, transform_input=False).type(dtype)\n",
    "    inception_model.eval();\n",
    "    up = nn.Upsample(size=(299, 299), mode='bilinear').type(dtype)\n",
    "    \n",
    "    def get_pred(x):\n",
    "        if resize:\n",
    "            x = up(x)\n",
    "        x = inception_model(x)\n",
    "        return F.softmax(x).data.cpu().numpy()\n",
    "\n",
    "    # Get predictions\n",
    "    preds = np.zeros((N, 1000))\n",
    "\n",
    "    for i, batch in enumerate(dataloader, 0):\n",
    "        batch = batch.type(dtype)\n",
    "        batchv = Variable(batch)\n",
    "        batch_size_i = batch.size()[0]\n",
    "\n",
    "        preds[i*batch_size:i*batch_size + batch_size_i] = get_pred(batchv)\n",
    "\n",
    "    # Now compute the mean kl-div\n",
    "    split_scores = []\n",
    "\n",
    "    for k in range(splits):\n",
    "        part = preds[k * (N // splits): (k+1) * (N // splits), :]\n",
    "        py = np.mean(part, axis=0)\n",
    "        scores = []\n",
    "        for i in range(part.shape[0]):\n",
    "            pyx = part[i, :]\n",
    "            scores.append(entropy(pyx, py))\n",
    "        split_scores.append(np.exp(np.mean(scores)))\n",
    "\n",
    "    return np.mean(split_scores), np.std(split_scores)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     class IgnoreLabelDataset(torch.utils.data.Dataset):\n",
    "#         def __init__(self, orig):\n",
    "#             self.orig = orig\n",
    "\n",
    "#         def __getitem__(self, index):\n",
    "#             return self.orig[index][0]\n",
    "\n",
    "#         def __len__(self):\n",
    "#             return len(self.orig)\n",
    "\n",
    "    # import torchvision.datasets as dset\n",
    "    # import torchvision.transforms as transforms\n",
    "\n",
    "    # cifar = dset.CIFAR10(root='data/', download=True,\n",
    "    #                          transform=transforms.Compose([\n",
    "    #                              transforms.Resize(32),\n",
    "    #                              transforms.ToTensor(),\n",
    "    #                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    #                          ])\n",
    "    # )\n",
    "\n",
    "    # IgnoreLabelDataset(cifar)\n",
    "\n",
    "    # print (\"Calculating Inception Score...\")\n",
    "    # print (inception_score(IgnoreLabelDataset(cifar), cuda=False, batch_size=32, resize=True, splits=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nothing\\AppData\\Local\\Temp\\ipykernel_11852\\1998552843.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x).data.cpu().numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def apply_transform_to_image(img_paths, transform):\n",
    "    result=[]\n",
    "    for img_path in img_paths:\n",
    "        pil_image = Image.open(img_path)\n",
    "        img_rgb = pil_image.convert('RGB')\n",
    "        transformed_image = transform(img_rgb)\n",
    "        result.append(transformed_image)\n",
    "    return torch.stack(result)\n",
    "\n",
    "# Example usage\n",
    "image_paths = ['./images/fake_image_0.png', './images/fake_image_1.png', './images/fake_image_2.png', './images/fake_image_3.png','./images/fake_image_4.png', './images/fake_image_5.png', './images/fake_image_6.png', './images/fake_image_7.png', './images/fake_image_8.png', './images/fake_image_9.png']\n",
    "\n",
    "# Define the transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "transformed_images = apply_transform_to_image(image_paths,transform)\n",
    "\n",
    "a,b=inception_score(transformed_images, cuda=False, batch_size=2, resize=True, splits=10)\n",
    "print(a,b)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
